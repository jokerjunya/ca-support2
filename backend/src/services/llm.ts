import axios from 'axios';
import { ParsedEmail } from '../types/gmail';

export interface LLMRequest {
  model: string;
  prompt: string;
  stream?: boolean;
  format?: 'json' | 'text';
  options?: {
    temperature?: number;
    top_p?: number;
    max_tokens?: number;
    stop?: string[];
  };
}

export interface LLMResponse {
  model: string;
  created_at: string;
  response: string;
  done: boolean;
  done_reason?: string;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export interface GenerateReplyRequest {
  originalEmail: ParsedEmail;
  replyType: 'business' | 'casual' | 'polite';
  customInstructions?: string;
  language?: 'ja' | 'en';
}

export interface GenerateReplyResponse {
  reply: string;
  tone: string;
  confidence: number;
  processing_time: number;
}

export class LLMService {
  private ollamaUrl: string;
  private defaultModel: string;
  private timeout: number;
  private maxRetries: number;

  constructor() {
    this.ollamaUrl = process.env.OLLAMA_URL || 'http://localhost:11434';
    this.defaultModel = process.env.OLLAMA_MODEL || 'qwen3:30b';
    this.timeout = parseInt(process.env.OLLAMA_TIMEOUT || '60000'); // 60ç§’
    this.maxRetries = parseInt(process.env.OLLAMA_MAX_RETRIES || '3');
  }

  /**
   * Ollama APIã¨ã®é€šä¿¡ãƒ†ã‚¹ãƒˆ
   */
  async testConnection(): Promise<boolean> {
    try {
      console.log(`ğŸ” Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆé–‹å§‹: ${this.ollamaUrl}/api/tags`);
      
      const response = await axios.get(`${this.ollamaUrl}/api/tags`, {
        timeout: 5000,
        headers: {
          'Content-Type': 'application/json'
        }
      });
      
      console.log(`âœ… Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ: ${response.status}`);
      console.log(`ğŸ“‹ åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«æ•°: ${response.data.models?.length || 0}`);
      
      if (response.data.models?.length > 0) {
        const modelNames = response.data.models.map((m: any) => m.name);
        console.log(`ğŸ“‹ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§: ${modelNames.join(', ')}`);
        
        if (modelNames.includes(this.defaultModel)) {
          console.log(`âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« ${this.defaultModel} ãŒåˆ©ç”¨å¯èƒ½`);
        } else {
          console.warn(`âš ï¸ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« ${this.defaultModel} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“`);
        }
      }
      
      return response.status === 200;
    } catch (error) {
      console.error('âŒ Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—:', error);
      
      if (axios.isAxiosError(error)) {
        console.error('âŒ Axiosã‚¨ãƒ©ãƒ¼è©³ç´°:');
        console.error(`   - ã‚³ãƒ¼ãƒ‰: ${error.code}`);
        console.error(`   - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ${error.message}`);
        console.error(`   - ãƒ¬ã‚¹ãƒãƒ³ã‚¹: ${error.response?.status} ${error.response?.statusText}`);
        console.error(`   - ãƒ‡ãƒ¼ã‚¿: ${JSON.stringify(error.response?.data)}`);
      }
      
      return false;
    }
  }

  /**
   * åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
   */
  async getAvailableModels(): Promise<string[]> {
    try {
      const response = await axios.get(`${this.ollamaUrl}/api/tags`, {
        timeout: 5000
      });
      return response.data.models?.map((model: any) => model.name) || [];
    } catch (error) {
      console.error('ãƒ¢ãƒ‡ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼:', error);
      return [];
    }
  }

  /**
   * LLMã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
   */
  async generateText(request: LLMRequest): Promise<LLMResponse> {
    const startTime = Date.now();
    
    try {
      console.log(`ğŸš€ LLM APIå‘¼ã³å‡ºã—é–‹å§‹:`);
      console.log(`   - URL: ${this.ollamaUrl}/api/generate`);
      console.log(`   - ãƒ¢ãƒ‡ãƒ«: ${request.model || this.defaultModel}`);
      console.log(`   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: ${request.prompt.length} æ–‡å­—`);
      console.log(`   - æ¸©åº¦: ${request.options?.temperature || 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'}`);
      console.log(`   - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ${this.timeout}ms`);
      
      // Ollama API ã®æ­£ç¢ºãªå½¢å¼ã«åˆã‚ã›ã‚‹
      const requestBody = {
        model: request.model || this.defaultModel,
        prompt: request.prompt,
        stream: false, // å¼·åˆ¶çš„ã«falseã«è¨­å®š
        options: {
          temperature: request.options?.temperature || 0.7,
          top_p: request.options?.top_p || 0.9,
          num_predict: request.options?.max_tokens || 1000
        }
      };
      
      // format ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‰Šé™¤ï¼ˆOllamaã§ã¯ä½¿ç”¨ã—ãªã„ï¼‰
      console.log(`ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£: ${JSON.stringify(requestBody, null, 2)}`);
      
      const response = await axios.post(
        `${this.ollamaUrl}/api/generate`,
        requestBody,
        {
          timeout: this.timeout,
          headers: {
            'Content-Type': 'application/json'
          }
        }
      );

      const processingTime = Date.now() - startTime;
      console.log(`âœ… LLM APIå‘¼ã³å‡ºã—æˆåŠŸ: ${processingTime}ms`);
      console.log(`ğŸ“¥ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: ${response.status}`);
      console.log(`ğŸ“¥ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: ${JSON.stringify(response.data).length} æ–‡å­—`);
      
      if (response.data.response) {
        console.log(`ğŸ“¥ ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆé•·: ${response.data.response.length} æ–‡å­—`);
        console.log(`ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: ${response.data.response.substring(0, 100)}...`);
      }
      
      return response.data;
    } catch (error) {
      const processingTime = Date.now() - startTime;
      console.error(`âŒ LLM APIå‘¼ã³å‡ºã—å¤±æ•— (${processingTime}ms):`, error);
      
      if (axios.isAxiosError(error)) {
        console.error('âŒ LLM Axiosã‚¨ãƒ©ãƒ¼è©³ç´°:');
        console.error(`   - ã‚³ãƒ¼ãƒ‰: ${error.code}`);
        console.error(`   - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ${error.message}`);
        console.error(`   - URL: ${error.config?.url}`);
        console.error(`   - ãƒ¬ã‚¹ãƒãƒ³ã‚¹: ${error.response?.status} ${error.response?.statusText}`);
        if (error.response?.data) {
          console.error(`   - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿: ${JSON.stringify(error.response.data)}`);
        }
      }
      
      throw new Error(`LLMç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: ${error}`);
    }
  }

  /**
   * ãƒ¡ãƒ¼ãƒ«è¿”ä¿¡ã‚’ç”Ÿæˆ
   */
  async generateReply(request: GenerateReplyRequest): Promise<GenerateReplyResponse> {
    const startTime = Date.now();
    
    try {
      console.log(`ğŸ¤– LLMè¿”ä¿¡ç”Ÿæˆé–‹å§‹: ${request.replyType}, ãƒ¢ãƒ‡ãƒ«: ${this.defaultModel}`);
      
      // æ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
      console.log(`ğŸ” Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...`);
      const isConnected = await this.testConnection();
      console.log(`ğŸ” Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆçµæœ: ${isConnected}`);
      
      if (!isConnected) {
        console.warn('âš ï¸ Ollamaæ¥ç¶šå¤±æ•— â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã‚’ä½¿ç”¨');
        return this.generateFallbackReply(request);
      }
      
      const prompt = this.buildReplyPrompt(request);
      console.log(`ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: ${prompt.length} æ–‡å­—`);
      console.log(`ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: ${prompt.substring(0, 100)}...`);
      
      console.log(`ğŸš€ Ollama APIå‘¼ã³å‡ºã—é–‹å§‹...`);
      const llmResponse = await this.generateText({
        model: this.defaultModel,
        prompt,
        format: 'text',
        options: {
          temperature: this.getTemperatureForTone(request.replyType),
          top_p: 0.9,
          max_tokens: 1000
        }
      });

      const processingTime = Date.now() - startTime;
      console.log(`âœ… LLMè¿”ä¿¡ç”Ÿæˆå®Œäº†: ${processingTime}ms`);
      console.log(`ğŸ“§ ç”Ÿæˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: ${llmResponse.response.length} æ–‡å­—`);
      
      // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å®Ÿéš›ã®è¿”ä¿¡å†…å®¹ã‚’æŠ½å‡º
      const reply = this.extractReplyFromResponse(llmResponse.response);
      console.log(`ğŸ“§ æŠ½å‡ºã•ã‚ŒãŸè¿”ä¿¡é•·: ${reply.length} æ–‡å­—`);
      console.log(`ğŸ“§ è¿”ä¿¡ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: ${reply.substring(0, 100)}...`);
      
      return {
        reply,
        tone: request.replyType,
        confidence: this.calculateConfidence(llmResponse),
        processing_time: processingTime
      };
    } catch (error) {
      console.error('âŒ è¿”ä¿¡ç”Ÿæˆã‚¨ãƒ©ãƒ¼:', error);
      console.error('âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°:', error instanceof Error ? error.stack : String(error));
      
      // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ
      return this.generateFallbackReply(request);
    }
  }

  /**
   * è¿”ä¿¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
   */
  private buildReplyPrompt(request: GenerateReplyRequest): string {
    const { originalEmail, replyType, customInstructions, language = 'ja' } = request;
    
    const toneInstructions = {
      business: 'ä¸å¯§ã§ãƒ“ã‚¸ãƒã‚¹ãƒ©ã‚¤ã‚¯ãªèªèª¿',
      casual: 'ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã§è¦ªã—ã¿ã‚„ã™ã„èªèª¿', 
      polite: 'éå¸¸ã«ä¸å¯§ã§æ•¬èªã‚’ä½¿ã£ãŸèªèª¿'
    };

    const languageInstruction = language === 'ja' ? 'æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚' : 'Please respond in English.';

    return `
ã‚ãªãŸã¯å„ªç§€ãªãƒ¡ãƒ¼ãƒ«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¡ãƒ¼ãƒ«ã«å¯¾ã™ã‚‹é©åˆ‡ãªè¿”ä¿¡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

enable_thinking=False

ã€å…ƒãƒ¡ãƒ¼ãƒ«æƒ…å ±ã€‘
ä»¶å: ${originalEmail.subject}
é€ä¿¡è€…: ${originalEmail.from}
å†…å®¹: ${originalEmail.body}

ã€è¿”ä¿¡æŒ‡ç¤ºã€‘
- èªèª¿: ${toneInstructions[replyType]}
- è¨€èª: ${languageInstruction}
- è¿”ä¿¡ã¯ç°¡æ½”ã§è¦ç‚¹ã‚’æŠ¼ã•ãˆãŸå†…å®¹ã«ã—ã¦ãã ã•ã„
- ç›¸æ‰‹ã®ãƒ¡ãƒ¼ãƒ«ã®å†…å®¹ã‚’ç†è§£ã—ã€é©åˆ‡ã«å¯¾å¿œã—ã¦ãã ã•ã„
${customInstructions ? `- è¿½åŠ æŒ‡ç¤º: ${customInstructions}` : ''}

ã€é‡è¦ã€‘
- è¿”ä¿¡å†…å®¹ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„
- æŒ¨æ‹¶ã‹ã‚‰å§‹ã‚ã¦ã€è¦ç‚¹ã‚’è¿°ã¹ã€é©åˆ‡ãªçµã³ã®æ–‡ã§çµ‚ã‚ã£ã¦ãã ã•ã„
- ä»¶åã¯å«ã‚ãªã„ã§ãã ã•ã„
- <think>ã‚¿ã‚°ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„

è¿”ä¿¡å†…å®¹:
`;
  }

  /**
   * èªèª¿ã«å¿œã˜ãŸæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
   */
  private getTemperatureForTone(replyType: string): number {
    switch (replyType) {
      case 'business':
        return 0.3; // ã‚ˆã‚Šä¿å®ˆçš„ã§ä¸€è²«æ€§ã®ã‚ã‚‹å¿œç­”
      case 'casual':
        return 0.7; // ã‚ˆã‚Šå‰µé€ çš„ã§è‡ªç„¶ãªå¿œç­”
      case 'polite':
        return 0.4; // ä¸å¯§ã ãŒä¿å®ˆçš„ãªå¿œç­”
      default:
        return 0.5;
    }
  }

  /**
   * LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰è¿”ä¿¡å†…å®¹ã‚’æŠ½å‡º
   */
  private extractReplyFromResponse(response: string): string {
    // <think>ã‚¿ã‚°ã‚’é™¤å»
    let cleanResponse = response.replace(/<think>[\s\S]*?<\/think>/g, '');
    
    // ä¸è¦ãªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
    cleanResponse = cleanResponse.replace(/^è¿”ä¿¡å†…å®¹:\s*/, '');
    cleanResponse = cleanResponse.replace(/^è¿”ä¿¡:\s*/, '');
    
    // ä½™åˆ†ãªæ”¹è¡Œã‚’æ•´ç†
    cleanResponse = cleanResponse.trim();
    
    return cleanResponse;
  }

  /**
   * ä¿¡é ¼åº¦ã‚’è¨ˆç®—
   */
  private calculateConfidence(llmResponse: LLMResponse): number {
    // ç°¡å˜ãªä¿¡é ¼åº¦è¨ˆç®—ï¼ˆã‚ˆã‚Šé«˜åº¦ãªãƒ­ã‚¸ãƒƒã‚¯ã«ç½®ãæ›ãˆå¯èƒ½ï¼‰
    const responseLength = llmResponse.response.length;
    const hasThinking = llmResponse.response.includes('<think>');
    
    let confidence = 0.7; // ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
    
    // é•·ã™ãã‚‹ãƒ»çŸ­ã™ãã‚‹å¿œç­”ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹
    if (responseLength > 50 && responseLength < 500) {
      confidence += 0.1;
    }
    
    // æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹
    if (hasThinking) {
      confidence += 0.1;
    }
    
    return Math.min(confidence, 1.0);
  }

  /**
   * ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ
   */
  private generateFallbackReply(request: GenerateReplyRequest): GenerateReplyResponse {
    console.log(`ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ: ${request.replyType}`);
    
    const fallbackReplies = {
      business: 'ãŠç–²ã‚Œæ§˜ã§ã™ã€‚\n\nã”é€£çµ¡ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚\nå†…å®¹ã‚’ç¢ºèªã•ã›ã¦ã„ãŸã ãã€å¾Œæ—¥æ”¹ã‚ã¦ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚\n\nã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚',
      casual: 'ãŠç–²ã‚Œæ§˜ï¼\n\nãƒ¡ãƒ¼ãƒ«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚\nç¢ºèªã—ã¦å¾Œã§è¿”ä¿¡ã—ã¾ã™ã­ã€‚',
      polite: 'ã„ã¤ã‚‚ãŠä¸–è©±ã«ãªã£ã¦ãŠã‚Šã¾ã™ã€‚\n\nã”ä¸å¯§ã«ã”é€£çµ¡ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚\nå†…å®¹ã‚’æ‹è¦‹ã—ã€æ”¹ã‚ã¦ã”é€£çµ¡ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚\n\nä½•å’ã‚ˆã‚ã—ããŠé¡˜ã„ç”³ã—ä¸Šã’ã¾ã™ã€‚'
    };

    return {
      reply: fallbackReplies[request.replyType],
      tone: request.replyType,
      confidence: 0.5,
      processing_time: 0
    };
  }
}

/**
 * LLMã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
 */
export const createLLMService = (): LLMService => {
  return new LLMService();
}; 